{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "verified-tradition",
   "metadata": {},
   "source": [
    "## Step 1: Load your data, including testing/training split from Project 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "completed-mailman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial code copied from Project 1\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"oasis_longitudinal.csv\")\n",
    "\n",
    "# First, drop NaN value rows on the non-split data to keep it balanced for the split\n",
    "# Drop rows with null/missing values in SES and MMSE columns (identified by using data.info())\n",
    "data = data.dropna(axis=0)\n",
    "\n",
    "# Define the predictor and the feature set\n",
    "X = data.drop('Group', axis=1)\n",
    "y = data['Group']\n",
    "\n",
    "# Finally, divide into training and test sets, add random_state\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-exemption",
   "metadata": {},
   "source": [
    "## Step 2: (If not already done in Project 1) Prepare your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "elder-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code copied from Project 1\n",
    "\n",
    "# Get rid of unecessary features: handedness (since all are right handed), MR delay time, Subject ID, MRI ID, Visit number\n",
    "X_train = X_train.drop(['Hand', 'MR Delay', 'Subject ID', 'MRI ID', 'Visit'], axis=1)\n",
    "X_test = X_test.drop(['Hand', 'MR Delay', 'Subject ID', 'MRI ID', 'Visit'], axis=1)\n",
    "\n",
    "# Convert \"M/F\" categorical attribute to int. F is class '0' and M is class '1'.\n",
    "X_train['M/F'].replace(['M', 'F'], [0, 1], inplace=True)\n",
    "X_test['M/F'].replace(['M', 'F'], [0, 1], inplace=True)\n",
    "\n",
    "# Since we are interested in predicting dementia based on biological attributes, not conversion to it, \n",
    "# transform \"Converted\" predictor category to \"Demented.\"\n",
    "y_train = y_train.str.replace('Converted', 'Demented')\n",
    "y_test = y_test.str.replace('Converted', 'Demented')\n",
    "\n",
    "# Encode the categorical predictor \"Demented\" to class '0' vs \"Nondemented\" to class '1'.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "# Feature scaling. Decided to use StandardScaler instead of MinMaxScaler so that \n",
    "# the mean was 0 and standard deviation constant i.e. create Gaussian data.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "X_train_scaled = ss.fit_transform(X_train)\n",
    "X_test_scaled = ss.fit_transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-quest",
   "metadata": {},
   "source": [
    "## Step 3: Examine your target attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "classified-exclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nondemented    0.536723\n",
      "Demented       0.463277\n",
      "Name: Group, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Transform \"Converted\" to \"Demented\" for the pre-split data\n",
    "y = y.str.replace('Converted', 'Demented')\n",
    "\n",
    "# Plot the categorical target variable from the pre-split data\n",
    "sns.countplot(x=y)\n",
    "\n",
    "# Show frequencies. Each outcome is approximately equally represented.\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-sequence",
   "metadata": {},
   "source": [
    "## Step 4: Select two of the following supervised learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interracial-casino",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First algorithm: Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate and fit the LogisticRegression model. When running the Logistic Regression algorithm, I got a \n",
    "# convergence error/warning (for lbfgs solver), though it still ran. Default iterations were 100, so I increased to 1000 and\n",
    "# that resolved the issue, but notably, only after scaling the data. Apparently higher iterations or failing to converge\n",
    "# reduces the confidence in the prediction and scoring.\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Second algorithm: Decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Instantiate and fit the Decision Tree Classifier. Set random_state to 2 for reproducibility.\n",
    "dectree = DecisionTreeClassifier(random_state=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-amber",
   "metadata": {},
   "source": [
    "## Step 5: For each of your selected models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "photographic-looking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression scoring metrics using default parameters:\n",
      "\n",
      "\tCross validation scores: [0.9122807  0.94736842 0.92982456 0.96428571 0.89285714]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95        33\n",
      "           1       0.93      1.00      0.96        38\n",
      "\n",
      "    accuracy                           0.96        71\n",
      "   macro avg       0.96      0.95      0.96        71\n",
      "weighted avg       0.96      0.96      0.96        71\n",
      "\n",
      "\tAccuracy score on un-scaled training data: 0.5371024734982333\n",
      "\tAccuracy score on un-scaled test data: 0.5352112676056338\n",
      "\tAccuracy score on SCALED training data: 0.9399293286219081\n",
      "\tAccuracy score on SCALED test data: 0.9577464788732394\n",
      "\n",
      "\tBest C from GridSearch: {'C': 1}\n",
      "\tBest score from GridSearch: 0.9398496240601505\n",
      "\n",
      "\n",
      "\n",
      "Decision tree scoring metrics using default parameters:\n",
      "\n",
      "\tCross validation scores: [0.89473684 0.89473684 0.92982456 0.89285714 0.83928571]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94        33\n",
      "           1       0.97      0.92      0.95        38\n",
      "\n",
      "    accuracy                           0.94        71\n",
      "   macro avg       0.94      0.95      0.94        71\n",
      "weighted avg       0.95      0.94      0.94        71\n",
      "\n",
      "\tAccuracy score on training data: 1.0\n",
      "\tAccuracy score on test data: 0.9436619718309859\n",
      "\n",
      "\tBest parameters from GridSearch: {'criterion': 'entropy', 'max_depth': 5, 'max_features': 8}\n",
      "\tBest score from GridSearch: 0.943358395989975\n",
      "\n",
      "\n",
      "Re-run the model with the best parameters from Grid Search\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95        33\n",
      "           1       0.93      1.00      0.96        38\n",
      "\n",
      "    accuracy                           0.96        71\n",
      "   macro avg       0.96      0.95      0.96        71\n",
      "weighted avg       0.96      0.96      0.96        71\n",
      "\n",
      "\tAccuracy score on training data: 0.9575971731448764\n",
      "\tAccuracy score on test data: 0.9577464788732394\n"
     ]
    }
   ],
   "source": [
    "# Import tools for cross validation and scoring\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# FIRST, FOR LOGISTIC REGRESSION. Run with the default parameters using cross-validation.\n",
    "print(\"Logistic regression scoring metrics using default parameters:\")\n",
    "print(\"\\n\\tCross validation scores: {}\\n\".format(cross_val_score(logreg, X_train, y_train, cv=5)))\n",
    "\n",
    "# Fit model to scaled training data and predict against the test data.\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate precision, recall, F1. Use classification_report to show a table of the metrics.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# CANNOT calculate r2, RMSE, and MAE for logistic regression, but can accuracy scores. Note how scaling vastly improved scores.\n",
    "print(\"\\tAccuracy score on un-scaled training data: {}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"\\tAccuracy score on un-scaled test data: {}\".format(logreg.score(X_test, y_test)))\n",
    "print(\"\\tAccuracy score on SCALED training data: {}\".format(logreg.score(X_train_scaled, y_train)))\n",
    "print(\"\\tAccuracy score on SCALED test data: {}\".format(logreg.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Grid search using the inverse regularization strength C (default = 1).\n",
    "param_grid = {'C': [0.001, 0.01, 1, 10, 100]}\n",
    "logregGS = GridSearchCV(logreg, param_grid=param_grid)\n",
    "logregGS.fit(X_train_scaled, y_train)\n",
    "print('\\n\\tBest C from GridSearch: {}'.format(logregGS.best_params_))\n",
    "print('\\tBest score from GridSearch: {}'.format(logregGS.best_score_))\n",
    "\n",
    "# Note: did not re-run model after Grid Search because best parameter was C = 1, which is the default C for\n",
    "# sklearn's Logistic Regression.\n",
    "\n",
    "\n",
    "# SECOND, FOR DECISION TREE CLASSIFIER. Perform cross-validation with default parameters. Use unscaled training data.\n",
    "print(\"\\n\\n\\nDecision tree scoring metrics using default parameters:\")\n",
    "print(\"\\n\\tCross validation scores: {}\\n\".format(cross_val_score(dectree, X_train, y_train, cv=5)))\n",
    "\n",
    "# Fit dectree to training data and predict against the test data.\n",
    "dectree.fit(X_train, y_train)\n",
    "y_pred = dectree.predict(X_test)\n",
    "\n",
    "# Calculate precision, recall, F1. Use classification_report to show a table of the metrics.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Accuracy scoring.\n",
    "print(\"\\tAccuracy score on training data: {}\".format(dectree.score(X_train, y_train)))\n",
    "print(\"\\tAccuracy score on test data: {}\".format(dectree.score(X_test, y_test)))\n",
    "\n",
    "# Grid search using DecisionTree parameters of 'criterion,' 'max_depth', and 'max_features'\n",
    "param_grid = {'criterion': ['entropy', 'gini'], 'max_depth': list(range(1, 9)), 'max_features': list(range(1, 10))}\n",
    "dectreeGS = GridSearchCV(dectree, param_grid=param_grid)\n",
    "dectreeGS.fit(X_train, y_train)\n",
    "print('\\n\\tBest parameters from GridSearch: {}'.format(dectreeGS.best_params_))\n",
    "print('\\tBest score from GridSearch: {}'.format(dectreeGS.best_score_))\n",
    "\n",
    "# Re-run the model with the best parameters\n",
    "dectree_best = DecisionTreeClassifier(criterion = 'entropy', max_depth=5, max_features=8, random_state=2)\n",
    "\n",
    "dectree_best.fit(X_train, y_train)\n",
    "y_pred = dectree_best.predict(X_test)\n",
    "\n",
    "# New classification report\n",
    "print('\\n\\nRe-run the model with the best parameters from Grid Search\\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# New accuracy scoring.\n",
    "print(\"\\tAccuracy score on training data: {}\".format(dectree_best.score(X_train, y_train)))\n",
    "print(\"\\tAccuracy score on test data: {}\".format(dectree_best.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-manual",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-tokyo",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
